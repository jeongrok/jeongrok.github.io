---
layout: single
title:  "Transformer Math 101"
categories: LLM
tags: [LLM]
---

### Compute Requirements

$C \approx \tao T \eq 6 PD$

where $C_{forward} \approx 2 PD$ and $C_{backward} \approx 4 PD$

- Actual FLOPs are different than the advertised FLOPs in GPU accelerator papers. Theoretical FLOPs never are achieved in practice (especially in distributed settings).

- "Chinchilla Scaling Laws" propose that $D=20P$ is the optimal number of parameters and the size of data, but this only counts where using 1000 GPUs for 1 hour and 1 GPU for 1000 hours cost you the same amount (when your goal is to maximize performance while minimizing the cost in GPU hours to train).

- However, some opinions recommend determining what inference cost is acceptable for your use case and training the largest model you can after staying under the inference budget.

- For any compute under 115 TFLOP/s/A100, you should safely assume that there is something wrong with your model or hardware configuration.

### Memory Requirements

- You need to know how much space in bytes the model will take up. This can tell you how large a model will fit on your local GPU for inference, or how large a model you can train across your cluster with a certain amount of total accelerator memory.

- Most transformers are trained on mixed precision, either fp15 + fp32 or bf16 + fp 32, which cuts down the amount of memory required to train a model.

- int8 memory = $(1 byte/ param) * No.Params$
- fp16, bf16 memory = $(2 byte/ param) * No.Params$
- fp 32 memory = $(4 byte/ param) * No.Params$

- Automatic Mixed Precision for Deep Learning: training half precision while maintaining the network accuracy achieved with single precision. This speeds up math-intensive operations, such as linear and convolution layers, memory-limited operations by assessing half the bytes compared to single-precision, and reduces memory requirements for training models, enabling larger models or minibatches.

- 
